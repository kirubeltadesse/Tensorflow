{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic on Theano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minimal Training Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =theano.tensor.fvector('x')\n",
    "target = theano.tensor.fscalar('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = theano.shared(numpy.asarray([0.2, 0.7]), 'W')\n",
    "y = (x * W).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = theano.tensor.sqr(target - y)\n",
    "gradients = theano.tensor.grad(cost, [W])\n",
    "W_updated = W - (0.1 * gradients[0])\n",
    "\n",
    "updates = [(W, W_updated)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = theano.function([x, target], y, updates=updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.99930167373795\n",
      "19.99958100424277\n",
      "19.99974860254566\n",
      "19.999849161527397\n",
      "19.999909496916437\n",
      "19.999945698149862\n",
      "19.999967418889916\n",
      "19.99998045133395\n",
      "19.99998827080037\n",
      "19.99999296248022\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    output = f([1.0, 1.0], 20.0)\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = theano.shared(numpy.asarray([[1.0,2.0],[3.0,4.0]]), 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "a"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 2.],\n",
       "       [3., 4.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemwise{true_div,no_inplace}.0\n"
     ]
    }
   ],
   "source": [
    "c = ((a + a) / 4.0)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = theano.tensor.nnet.sigmoid(a)\n",
    "c = theano.tensor.tanh(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = theano.tensor.nnet.softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.sum()\n",
    "c = a.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.max()\n",
    "c = a.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = theano.tensor.argmax(a)\n",
    "c = theano.tensor.argmax(a, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We sometimes need to change the dimensions of a tensor and reshape() allows us to do that. It takes as input a tuple \n",
    "# containing the new shape and returns a new tensor with that shape. In the first example below, we shape a square matrix \n",
    "# into a 1x4 matrix. In the second example, we use-1 which means \"as big as the dimension needs to be\".\n",
    "a = theano.shared(numpy.asarray([[1,2], [3,4]]), 'a')\n",
    "c = a.reshape((1,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a.reshape((-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = theano.tensor.zeros_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.eval()\n",
    " \n",
    "c = a.dimshuffle((1,0))\n",
    " \n",
    "c = a.dimshuffle(('x',0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = theano.shared(numpy.asarray([[1.0,2.0],[3.0,4.0]]), 'a')\n",
    "b = [1,1,0]\n",
    "c = a[b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import sys\n",
    "import numpy\n",
    "import collections\n",
    "\n",
    "floatX=theano.config.floatX\n",
    "\n",
    "class Classifier(object):\n",
    "    def __init__(self, n_features):\n",
    "        # network parameters\n",
    "        random_seed = 42\n",
    "        hidden_layer_size = 5\n",
    "        l2_regularisation = 0.001\n",
    "\n",
    "        # random number generator\n",
    "        rng = numpy.random.RandomState(random_seed) \n",
    "\n",
    "        # setting up variables for the network\n",
    "        input_vector = theano.tensor.fvector('input_vector')\n",
    "        target_value = theano.tensor.fscalar('target_value')\n",
    "        learningrate = theano.tensor.fscalar('learningrate')\n",
    "\n",
    "        # input->hidden weights\n",
    "        W_hidden_vals = numpy.asarray(rng.normal(loc=0.0, scale=0.1, size=(n_features, hidden_layer_size)), dtype=floatX)\n",
    "        W_hidden = theano.shared(W_hidden_vals, 'W_hidden')\n",
    "\n",
    "        # calculating the hidden layer\n",
    "        hidden = theano.tensor.dot(input_vector, W_hidden)\n",
    "        hidden = theano.tensor.nnet.sigmoid(hidden)\n",
    "\n",
    "        # hidden->output weights\n",
    "        W_output_vals = numpy.asarray(rng.normal(loc=0.0, scale=0.1, size=(hidden_layer_size, 1)), dtype=floatX)\n",
    "        W_output = theano.shared(W_output_vals, 'W_output')\n",
    "\n",
    "        # calculating the predicted value (output)\n",
    "        predicted_value = theano.tensor.dot(hidden, W_output)\n",
    "        predicted_value = theano.tensor.nnet.sigmoid(predicted_value)\n",
    "\n",
    "        # calculating the cost function\n",
    "        cost = theano.tensor.sqr(predicted_value - target_value).sum()\n",
    "        cost += l2_regularisation * (theano.tensor.sqr(W_hidden).sum() + theano.tensor.sqr(W_output).sum())\n",
    "\n",
    "        # calculating gradient descent updates based on the cost function\n",
    "        params = [W_hidden, W_output]\n",
    "        gradients = theano.tensor.grad(cost, params)\n",
    "        updates = [(p, p - (learningrate * g)) for p, g in zip(params, gradients)]\n",
    "\n",
    "        # defining Theano functions for training and testing the network\n",
    "        self.train = theano.function([input_vector, target_value, learningrate], [cost, predicted_value], updates=updates, allow_input_downcast=True)\n",
    "        self.test = theano.function([input_vector, target_value], [cost, predicted_value], allow_input_downcast=True)\n",
    "\n",
    "def read_dataset(path):\n",
    "    \"\"\"Read a dataset, with each line containing a real-valued label and a feature vector\"\"\"\n",
    "    dataset = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line_parts = line.strip().split()\n",
    "            label = float(line_parts[0])\n",
    "            vector = numpy.array([float(line_parts[i]) for i in xrange(1, len(line_parts))])\n",
    "            dataset.append((label, vector))\n",
    "    return dataset\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path_train = sys.argv[1]\n",
    "    path_test = sys.argv[2]\n",
    "\n",
    "    # training parameters\n",
    "    learningrate = 0.1\n",
    "    epochs = 10\n",
    "\n",
    "    # reading the datasets\n",
    "    data_train = read_dataset(path_train)\n",
    "    data_test = read_dataset(path_test)\n",
    "\n",
    "    # creating the network\n",
    "    n_features = len(data_train[0][1])\n",
    "    classifier = Classifier(n_features)\n",
    "\n",
    "    # training\n",
    "    for epoch in range(epochs):\n",
    "        cost_sum = 0.0\n",
    "        correct = 0\n",
    "        for label, vector in data_train:\n",
    "            cost, predicted_value = classifier.train(vector, label, learningrate)\n",
    "            cost_sum += cost\n",
    "            if (label == 1.0 and predicted_value >= 0.5) or (label == 0.0 and predicted_value < 0.5):\n",
    "                correct += 1\n",
    "        print(\"Epoch: \" + str(epoch) + \", Training_cost: \" + str(cost_sum) + \", Training_accuracy: \" + str(float(correct) / len(data_train)))\n",
    "\n",
    "    # testing\n",
    "    cost_sum = 0.0\n",
    "    correct = 0\n",
    "    for label, vector in data_test:\n",
    "        cost, predicted_value = classifier.test(vector, label)\n",
    "        cost_sum += cost\n",
    "        if (label == 1.0 and predicted_value >= 0.5) or (label == 0.0 and predicted_value < 0.5):\n",
    "            correct += 1\n",
    "    print(\"Test_cost: \" + str(cost_sum) + \", Test_accuracy: \" + str(float(correct) / len(data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import theano\n",
    "import collections\n",
    "import numpy\n",
    "import random\n",
    "\n",
    "floatX=theano.config.floatX\n",
    "\n",
    "class RnnClassifier(object):\n",
    "    def __init__(self, n_words, n_classes):\n",
    "        # network parameters\n",
    "        random_seed = 42\n",
    "        word_embedding_size = 200\n",
    "        recurrent_size = 100\n",
    "        l2_regularisation = 0.0001\n",
    "\n",
    "        # random number generator\n",
    "        self.rng = numpy.random.RandomState(random_seed)\n",
    "\n",
    "        # this is where we keep shared weights that are optimised during training\n",
    "        self.params = collections.OrderedDict()\n",
    "\n",
    "        # setting up variables for the network\n",
    "        input_indices = theano.tensor.ivector('input_indices')\n",
    "        target_class = theano.tensor.iscalar('target_class')\n",
    "        learningrate = theano.tensor.fscalar('learningrate')\n",
    "\n",
    "        # creating the matrix of word embeddings\n",
    "        word_embeddings = self.create_parameter_matrix('word_embeddings', (n_words, word_embedding_size))\n",
    "\n",
    "        # extract the relevant word embeddings, given the input word indices\n",
    "        input_vectors = word_embeddings[input_indices]\n",
    "\n",
    "        # gated recurrent unit\n",
    "        # from: Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation (Cho et al, 2014)\n",
    "        def gru_step(x, h_prev, W_xm, W_hm, W_xh, W_hh):\n",
    "            m = theano.tensor.nnet.sigmoid(theano.tensor.dot(x, W_xm) + theano.tensor.dot(h_prev, W_hm))\n",
    "            r = _slice(m, 0, 2)\n",
    "            z = _slice(m, 1, 2)\n",
    "            _h = theano.tensor.tanh(theano.tensor.dot(x, W_xh) + theano.tensor.dot(r * h_prev, W_hh))\n",
    "            h = z * h_prev + (1.0 - z) * _h\n",
    "            return h\n",
    "\n",
    "        W_xm = self.create_parameter_matrix('W_xm', (word_embedding_size, recurrent_size*2))\n",
    "        W_hm = self.create_parameter_matrix('W_hm', (recurrent_size, recurrent_size*2))\n",
    "        W_xh = self.create_parameter_matrix('W_xh', (word_embedding_size, recurrent_size))\n",
    "        W_hh = self.create_parameter_matrix('W_hh', (recurrent_size, recurrent_size))\n",
    "        initial_hidden_vector = theano.tensor.alloc(numpy.array(0, dtype=floatX), recurrent_size)\n",
    "\n",
    "        hidden_vector, _ = theano.scan(\n",
    "            gru_step,\n",
    "            sequences = input_vectors,\n",
    "            outputs_info = initial_hidden_vector,\n",
    "            non_sequences = [W_xm, W_hm, W_xh, W_hh]\n",
    "        )\n",
    "        hidden_vector = hidden_vector[-1]\n",
    "\n",
    "        # hidden->output weights\n",
    "        W_output = self.create_parameter_matrix('W_output', (n_classes,recurrent_size))\n",
    "        output = theano.tensor.nnet.softmax([theano.tensor.dot(W_output, hidden_vector)])[0]\n",
    "        predicted_class = theano.tensor.argmax(output)\n",
    "\n",
    "        # calculating the cost function\n",
    "        cost = -1.0 * theano.tensor.log(output[target_class])\n",
    "        for m in self.params.values():\n",
    "            cost += l2_regularisation * (theano.tensor.sqr(m).sum())\n",
    "\n",
    "        # calculating gradient descent updates based on the cost function\n",
    "        gradients = theano.tensor.grad(cost, self.params.values())\n",
    "        updates = [(p, p - (learningrate * g)) for p, g in zip(self.params.values(), gradients)]\n",
    "\n",
    "        # defining Theano functions for training and testing the network\n",
    "        self.train = theano.function([input_indices, target_class, learningrate], [cost, predicted_class], updates=updates, allow_input_downcast = True)\n",
    "        self.test = theano.function([input_indices, target_class], [cost, predicted_class], allow_input_downcast = True)\n",
    "\n",
    "    def create_parameter_matrix(self, name, size):\n",
    "        \"\"\"Create a shared variable tensor and save it to self.params\"\"\"\n",
    "        vals = numpy.asarray(self.rng.normal(loc=0.0, scale=0.1, size=size), dtype=floatX)\n",
    "        self.params[name] = theano.shared(vals, name)\n",
    "        return self.params[name]\n",
    "\n",
    "\n",
    "def _slice(M, slice_num, total_slices):\n",
    "    \"\"\" Helper function for extracting a slice from a tensor\"\"\"\n",
    "    if M.ndim == 3:\n",
    "        l = M.shape[2] / total_slices\n",
    "        return M[:, :, slice_num*l:(slice_num+1)*l]\n",
    "    elif M.ndim == 2:\n",
    "        l = M.shape[1] / total_slices\n",
    "        return M[:, slice_num*l:(slice_num+1)*l]\n",
    "    elif M.ndim == 1:\n",
    "        l = M.shape[0] / total_slices\n",
    "        return M[slice_num*l:(slice_num+1)*l]\n",
    "\n",
    "def read_dataset(path):\n",
    "    \"\"\"Read a dataset, where the first column contains a real-valued score,\n",
    "    followed by a tab and a string of words.\n",
    "    \"\"\"\n",
    "    dataset = []\n",
    "    with open(path, \"r\") as f:\n",
    "        for line in f:\n",
    "            line_parts = line.strip().split(\"\\t\")\n",
    "            dataset.append((float(line_parts[0]), line_parts[1].lower()))\n",
    "    return dataset\n",
    "\n",
    "def score_to_class_index(score, n_classes):\n",
    "    \"\"\"Maps a real-valued score between [0.0, 1.0] to a class id, given n_classes.\"\"\"\n",
    "    for i in xrange(n_classes):\n",
    "        if score <= (i + 1.0) * (1.0 / float(n_classes)):\n",
    "            return i\n",
    "\n",
    "def create_dictionary(sentences, min_freq):\n",
    "    \"\"\"Creates a dictionary that maps words to ids.\n",
    "    If min_freq is positive, removes all words that have a smaller frequency.\n",
    "    \"\"\"\n",
    "    counter = collections.Counter()\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            counter.update([word])\n",
    "\n",
    "    word2id = collections.OrderedDict()\n",
    "    word2id[\"<unk>\"] = 0\n",
    "    word2id[\"<s>\"] = 1\n",
    "    word2id[\"</s>\"] = 2\n",
    "\n",
    "    word_count_list = counter.most_common()\n",
    "    for (word, count) in word_count_list:\n",
    "        if min_freq < 0 or count >= min_freq:\n",
    "            word2id[word] = len(word2id)\n",
    "\n",
    "    return word2id\n",
    "\n",
    "def sentence2ids(words, word2id):\n",
    "    \"\"\"Takes a list of words and converts them to ids using the word2id dictionary.\"\"\"\n",
    "    ids = [word2id[\"<s>\"],]\n",
    "    for word in words:\n",
    "        if word in word2id:\n",
    "            ids.append(word2id[word])\n",
    "        else:\n",
    "            ids.append(word2id[\"<unk>\"])\n",
    "    ids.append(word2id[\"</s>\"])\n",
    "    return ids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path_train = sys.argv[1]\n",
    "    path_test = sys.argv[2]\n",
    "\n",
    "    # training parameters\n",
    "    min_freq = 2\n",
    "    epochs = 3\n",
    "    learningrate = 0.1\n",
    "    n_classes = 5\n",
    "\n",
    "    # reading the datasets\n",
    "    sentences_train = read_dataset(path_train)\n",
    "    sentences_test = read_dataset(path_test)\n",
    "\n",
    "    # creating the dictionary from the training data\n",
    "    word2id = create_dictionary([sentence.split() for label, sentence in sentences_train], min_freq)\n",
    "\n",
    "    # mapping training and test data to the dictionary indices\n",
    "    data_train = [(score_to_class_index(score, n_classes), sentence2ids(sentence.split(), word2id)) for score, sentence in sentences_train]\n",
    "    data_test = [(score_to_class_index(score, n_classes), sentence2ids(sentence.split(), word2id)) for score, sentence in sentences_test]\n",
    "\n",
    "    # shuffling the training data\n",
    "    random.seed(1)\n",
    "    random.shuffle(data_train)\n",
    "\n",
    "    # creating the classifier\n",
    "    rnn_classifier = RnnClassifier(len(word2id), n_classes)\n",
    "\n",
    "    # training\n",
    "    for epoch in range(epochs):\n",
    "        cost_sum = 0.0\n",
    "        correct = 0\n",
    "        for target_class, sentence in data_train:\n",
    "            cost, predicted_class = rnn_classifier.train(sentence, target_class, learningrate)\n",
    "            cost_sum += cost\n",
    "            if predicted_class == target_class:\n",
    "                correct += 1\n",
    "        print(\"Epoch: \" + str(epoch) + \"\\tCost: \" + str(cost_sum) + \"\\tAccuracy: \" + str(float(correct)/len(data_train)))\n",
    "\n",
    "\n",
    "    # testing\n",
    "    cost_sum = 0.0\n",
    "    correct = 0\n",
    "    for target_class, sentence in data_test:\n",
    "        cost, predicted_class = rnn_classifier.test(sentence, target_class)\n",
    "        cost_sum += cost\n",
    "        if predicted_class == target_class:\n",
    "            correct += 1\n",
    "    print(\"Test_cost: \" + str(cost_sum) + \"\\tTest_accuracy: \" + str(float(correct)/len(data_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_step(x, previous_hidden_vector, W_input, W_recurrent):\n",
    "    hidden_vector = theano.tensor.dot(x, W_input) + theano.tensor.dot(previous_hidden_vector, W_recurrent)\n",
    "    hidden_vector = theano.tensor.nnet.sigmoid(hidden_vector)\n",
    "    \n",
    "W_input = self.create_parameter_matrix('W_input', (word_embedding_size, recurrent_size))\n",
    "W_recurrent = self.create_parameter_matrix('W_recurrent', (recurrent_size, recurrent_size))\n",
    "initial_hidden_vector = theano.tensor.alloc(numpy.array(0, dtype=floatX), recurrent_size)\n",
    " \n",
    "hidden_vector, _ = theano.scan(\n",
    "    rnn_step,\n",
    "    sequences = input_vectors,\n",
    "    outputs_info = initial_hidden_vector,\n",
    "    non_sequences = [W_input, W_recurrent]\n",
    ")\n",
    " \n",
    "hidden_vector = hidden_vector[-1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
